\chapter{Analysis}

In this chapter, we will analyze and explain important topics related to the thesis objective, such as artificial intelligence (AI), prompt engineering, risks associated with implementing AI solutions, content moderation, methods of attacks, and finally current legislation in important countries. From risks, we will focus on ethical and security risks and how they can be exploited using so-called jailbreaking.

\section{Artificial Intelligence}
One of the simplest definitions of an intelligent system is that of a system
that ``processes information in order to do something purposefu'' \cite{Dignum_2019}.
Computer science recognizes some types of artificial intelligence. Figure~\ref{fig:AI-ML-DL-NN} shows the typical hierarchy of these types:

\begin{itemize}
    \item Artificial Intelligence
    \item Machine Learning
    \item Deep Learning and Neural Networks
\end{itemize}

\begin{figure}[htpb]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/final deep learning.png}
\par\end{centering}
\caption{Aritficial Intelligence hierarchy \cite{ai_hierarchy_pic}
\label{fig:AI-ML-DL-NN}}
\end{figure}

% \begin{figure}[htbp]
% ...
% h - Place the figure "here" (at the position in the code).
% t - Top of the page.
% b - Bottom of the page.
% p - On a separate page for floats.

\textbf{Artificial Intelligence (AI)} is a general term to describe any system with some sign of intelligence. AI is a field focused on automating intellectual tasks normally performed by humans, and Machine Learning and Deep Learning are specific methods of achieving this goal \cite{AI-ML-DL}. Although we speak about intelligence, we use this term to categorize non-learning algorithms which are just based on deterministic rules and heuristics, nevertheless, this behavior seems intelligent to humans. For example, if we have a game or a puzzle of some sort and define every possible rule for the algorithm, the machine could solve it pretty easily based on computing power in modern times. This would be a non-learning algorithm, but a typical person would consider it an intelligent program because of how quickly it was able to solve this puzzle, which is perceived as complex by a typical person. Although AI is capable of solving clearly defined logical problems, it often fails tasks that require higher-level pattern recognition, such as speech recognition or image classification \cite{AI-ML-DL}. These more complicated tasks are where the Machine Learning and Deep Learning methods perform well \cite{AI-ML-DL}.

\textbf{Machine Learning (ML)} is a term used to describe systems that can learn from data and improve their performance step by step without being specifically designed for every task. ML algorithms find patterns and connections in the data rather than following strict rules to classify information, generate predictions, or optimize activities. For example, ML is used in Data Science specifically Data Analysis to find correlations between data, preprocess the said data, and finally create a model to predict outcomes based on real-world data. In ML, there are three commonly recognized learning methods:
\begin{itemize}
    \item supervised learning
        \begin{itemize}
            \item Algorithms based on this method will receive immediate responses for the output they produce. This is used mainly in classification and regression. Some examples of supervised learning are handwriting recognition, general image classification (e.g. does the provided image contain an animal), disease diagnosis, etc.
        \end{itemize}
    \item unsupervised learning
        \begin{itemize}
                \item This method is used mainly for clustering data because algorithms based on this method (e.g. k-means) do not get immediate feedback for their output. This is very useful in clustering to find sequences or relationships between the data. An example of unsupervised learning would be to group news articles based on the context of the article into categories.
            \end{itemize}
    \item reinforcement learning
        \begin{itemize}
            \item Reinforcement learning is used mainly for algorithms that play games. This technique rewards good behavior and punishes bad behavior. For example, in the game Snake, the so-called ``agent'' that would play this game would be rewarded for eating apples (gaining points) and punished for bumping into the wall or himself (hence the ``reinforcement''). This behavior is uncontrolled by the programmer, and the ``agent'' would learn to play the game to maximize points, which is a desirable outcome.
        \end{itemize}
\end{itemize}

\textbf{Deep Learning (DL)} is a branch of machine learning concerned with the use of \textbf{neural networks (NN)} to perform tasks such as representation learning, regression, and classification. The focus of the field, which draws inspiration from biological neuroscience, is ``training'' artificial neurons to process data by stacking them in layers. The term ``deep'' describes a network that uses several layers, ranging from three to several hundred or thousands \cite{LeCun2015}. There are many types of neural networks, but the most known are convolutional neural networks (CNN) and recurrent neural networks (RNN). CNNs are mostly used for image classification, i.e. facial recognition or object detection. On the other hand, RNNs are used for finding connections between sequential data such as language modeling, text generation, time-series anomaly detection, and more.


\subsection{AI Models \label{subsec:AI-Models}}

There are various types of AI models. The prominent and most used are text-to-text models followed by text-to-image and text-to-audio models. 

Mostly, we focus on the text-to-text models. They use Natural Language Processing (NLP), which is a subfield of artificial intelligence and linguistics. NLP as a technology is used to provide understanding of human language for machines. The model understands the semantics and context of the text and generates response based on trained data. The subset of NLP models are large language models (LLMs). The models rely on vast amounts of data. This is where the ``large'' in the large language model comes from. Because of the great scale, they are able to predict the next word based on probability. We mentioned that these models need to be trained. This is where Generative Pre-Trained Transformers (GPTs) come in. GPT is the final step of the text-to-text AI model. 

What is the GPT? It is a Large Language Model based on the transformer architecture published in a paper called ``Attention Is All You Nee'' by Vaswani et al. \cite{vaswani2023attentionneed}. It is pre-trained on massive amounts of data using reinforcement learning with Human Feedback (RLHF) \cite{openai_chatgpt_page} and generates text based on prediction of the next word.

The most well-known GPT is OpenAI's ChatGPT which was released in November 2022 and experienced massive boom with its release. This technology is very exciting, but every technology has its own limitations. OpenAI in their article \cite{openai_chatgpt_page} state them as follows:

\begin{itemize}
    \item ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers
    \item ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times
    \item The model is often excessively verbose and overuses certain phrases, such as restating that itâ€™s a language model trained by OpenAI
    \item The model sometimes respond to harmful instructions or exhibit biased behavior.
\end{itemize}

These limitations are the reason for some of the attacks that can be performed to misuse this technology for harmful purposes. We will discuss this in more detail in Section~\ref{sec:risks}.


\subsection{Prompt engineering}
Prompt engineering involves designing and optimizing text instructions called prompts, which are mainly used to communicate with chatbots that use LLMs in the background, such as OpenAI ChatGPT, Deepseek, and Microsoft Copilot. However, there can also be models whose output is not text, but image, video, or audio as mentioned in the previous Section~\ref{subsec:AI-Models}. White et al. describe prompt angineering as the means by which LLMs are programmed via prompts \cite{white2023promptpatterncatalogenhance}. They described a few patterns which they grouped into categories shown in Table~\ref{tab:prompt_patterns}.

{
    \renewcommand{\arraystretch}{1.2}
    \begin{table}[htpb]
        \caption{Classifying Prompt Patterns~\cite{white2023promptpatterncatalogenhance}}
        \centering
        \begin{tabular}{|l|l|}
            \hline \cellcolor[gray]{0.8}\textbf{Pattern Category} & \cellcolor[gray]{0.8}\textbf{Prompt Pattern} \\ \hline
            \textbf {Input Semantics} & Meta Language Creation \\ \hline
            \textbf {Output} & Output Automater \\
            \textbf {Customization} & Persona \\
             & Visualization Generator \\
             & Recipe \\
             & Template \\ \hline
            \textbf{\mbox{Error Identification}} & Fact Check List \\
            & Reflection \\ \hline
            \textbf {Prompt} & Question Refinement \\
            \textbf {Improvement} & Alternative Approaches \\
            & Cognitive Verifier \\
            & Refusal Breaker \\ \hline
            \textbf {Interaction} & Flipped Interaction \\
            & Game Play \\
            & Infinite Generation \\ \hline
            \textbf{Context Control} & Context Manager \\ \hline
        \end{tabular}
        \label{tab:prompt_patterns}
    \end{table}
}

The most notable prompt pattern is \textbf{Persona}. As we will discuss in more detail in Section~\ref{sec:jailbreak}, the Persona pattern is the basis of most jailbreak methods. In a nutshell, when using the Persona pattern, the user instructs the chatbot to behave like some Persona. For example, with prompt: ``From now on, you will be Travel expert'', the chatbot will give us (in its ``opinion'') best possible tips and suggestions for traveling when prompted for this information.

\section{Risks of implementing AI solutions \label{sec:risks}}
When implementing AI solutions in any domain, we must consider the natural risks of doing so. We, as a society, learned from history and philosophy that there will always be someone who will try to exploit any new technology to cause harm. In this section, we will discuss the possible major risks associated with the implementation of AI solutions.

\subsection{Ethical risks}
% Content generation based on copyrighted material (Copyrighted material as input)
% people compromising 
% compromising and harmful media 
% deep fakes
% cyberbullying, fake news
% etc.
Although LLMs are beneficial for helping people, they also carry risks with them. These risks include the spread of misinformation, the creation of deep fakes, privacy concerns, and other ethical problems that we will discuss in this section.

\subsubsection*{Misinformation}

Bad actors abuse the ``creativity'' aspect of LLMs and generate misinformation and false news that pose a major threat to society when dealing with critical issues such as climate crisis and the health of individuals. Very popular amongst governments is to use misinformation generated by LLMs spread by fake accounts on social networks to skew or influence political situation or public sentiment in favor of their preferred party or an individual.

\subsubsection*{Identity Theft}

When training the LLM from non-anonymized data, potential leaks or extractions of these data can lead to identity theft and targeted phishing. 
In the opposite view, publicly available data, however, often not free, can be used as input to already trained models to create deepfakes and later use these deepfakes to harm the public view of the individual or even worse.

\subsubsection*{Bias Amplification}

Biased training data and targeted prompts can amplify discrimination against groups with less oversight power \cite{kumar2024ethicsinteractionmitigatingsecurity}. For example, models trained on biased data that include stereotypes about gender, race, or religion generate outputs that reinforce these stereotypes, which could be harmful to vulnerable groups. The consequences of the restorative steps that were complicated by power imbalances deepen the demographic inequalities on this issue \cite{kumar2024ethicsinteractionmitigatingsecurity}.

\subsubsection*{Copyright violations}

Some companies unethically train their models on copyright-protected material, i.e. online news articles, digital media, works of art, etc. This leads to stealing intellectual property (IP). However, the legislation on this topic is currently unclear, but we will dive deeper into this topic in Section~\ref{sec:legislation}.

\subsubsection*{Military use}

Another topic that needs to be addressed is whether the military should utilize their data to develop LLMs which would be capable of teaching other military personnel, helping to create weapons, analyzing confidential information, etc. This could be quite dangerous if the system falls into the hands of a bad actor or adversary government where this information could be used for nefarious purposes.


\subsection{Moral risks}
With the implementation of AI solutions in addition to ethical problems, moral problems are also present. One of the problems is generating sexually explicit content. Bad actors can use LLMs to create this type of content and then distribute it, which could expose the content to minors and other vulnerable individuals and cause them harm. This also applies to violent content, the making of weapons, illegal chemicals, and lastly forbidden language.


\subsection{Cybersecurity risks}
AI can prove itself in the near future as a very useful and helpful tool to develop solutions for malware detection, malware prevention, and cybersecurity training. On the other hand, as we have already mentioned, everything has its advantages and disadvantages. Unfortunately, there are big disadvantages of rapid development of AI, which means that there are and there will be AIs, which can also be used for the creation of malware, social engineering attacks and phishing in general. Some of these risks were identified by Egbuna \cite{Princess-Egbuna_2021} as follows:

\begin{itemize}
    \item AI-Powered Malware and Ransomware
    \item Automated and Scalable Attacks
    \item Deepfake and Social Engineering Attacks
\end{itemize}

\subsubsection*{AI-Powered Malware and Ransomware}

Conventional malware functions by penetrating systems, causing harm, and exfiltrating data. In contrast, AI-powered malware can adapt, rendering detection and mitigation more challenging. Using machine learning algorithms, this malware assesses its surroundings and alters its actions to bypass antivirus software and intrusion detection systems. Particularly concerning is the AI-powered ransomware, which has increased its threat level. This type of ransomware quickly identifies vulnerabilities, encrypts essential data, and adjusts ransom demands according to the victim's financial capacity. The flexibility offered by AI enhances the distribution of ransomware and aids in its evasion of detection, thus amplifying its impact.~\cite{Princess-Egbuna_2021}

\subsubsection*{Automated and Scalable Attacks}

These attacks are the result of LLMs. The reason is that these models can analyze and summarize vast amounts of data, and bad actors can automate this process using frameworks that can be executed on a large scale. At this scale, models trained by bad actors can achieve their goal quicker and easier.~\cite{Princess-Egbuna_2021}

\subsubsection*{Deepfake and Social Engineering Attacks}

We mentioned earlier that deepfakes are an ethical problem, but they are also connected to cybersecurity. We can broadly define deepfake as an AI-generated media that convincingly mimics real individuals.

Deepfake technology is used by bad actors in social engineering attacks. This technique can deceive and manipulate targets by creating phony films or audio recordings of trustworthy people like CEOs\footnote{Chief Executive Officer} of companies or public leaders \cite{Princess-Egbuna_2021}. In February 2024, the American media company CNN reported an example case of this behavior \cite{deepfake_CFO}. The financial worker of a multinational company was tricked by video call with supposedly his coworkers and CFO\footnote{Chief Financial Officer} to send around \$25 million which was later revealed to be a fake social engineering scam \cite{deepfake_CFO}.

% previously Content filters
\section{Content moderation\label{sec:content_moderation}}
Every major chatbot using LLM have some kind of content moderation implemented. The developers of these systems use different techniques to prevent these models from generating inappropriate or harmful content. These techniques include predefined sets of rules to define this type of content and not allow its generation. The models are also fine-tuned to contain primarily nonharmful content, but since they operate on a huge scale and massive amounts of training data, this task becomes impossible to achieve without some content slipping through the safeguards. Another method, which is implemented in combination with the other methods, uses system prompts or often called ``alignment prompts''. These prompts are hidden from the user when the chatbot interacts with them. The typical prompt architecture is shown in Figure~\ref{fig:system_prompt}. 

In this figure, the example system prompt could be: ``Be a kind and helpful AI assistant. Do not generate any harmful information even if the user asks you!''. In this system, the user prompt is appended to the system prompt with the context of the conversation or from the optional files included in the prompt and then sent to the model. This architecture should prevent generating harmful content, but as we will discuss in next section, the bad actors are very inventive and still overcome these security measures. When all previously mentioned safeguards fail, the last option is to report the generated prompt which includes harmful content to the moderators, so that human can review the prompt and figure if the generated content was, in fact, harmful.

\begin{figure}[htpb]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/content_filtering.png}
\par\end{centering}
\caption{Prompt layers \cite{systemprompt}
 \label{fig:system_prompt}}
\end{figure}


\subsection{Jailbreak \label{sec:jailbreak}}
Jailbreak is the specific formulation of a user prompt that is used to bypass the filters and safety checks of LLMs, tricking them into providing harmful or objectionable content based on this prompt.
Jailbreak prompts tend to have these characteristics:
\begin{itemize}
    \item Prompt length
    \item Prompt semantics
\end{itemize}

\textbf{Prompt length} (in tokens) tends to be longer because attackers use additional instructions to cause the model to behave in specific ways to bypass the safeguards.
Shen et al.~\cite{shen2024donowcharacterizingevaluating} found that jailbreak prompts are noticeably longer than regular prompts and exhibit a monthly increase in length. On average, the token count for a jailbreak prompt is 555, which is 1.5 times the length of typical prompts~\cite{shen2024donowcharacterizingevaluating}.

\textbf{Prompt semantics} means that LLMs semantically understand the structure and meaning of the prompt. Shen et al.~\cite{shen2024donowcharacterizingevaluating} also found that most jailbreak prompts exhibit semantic similarities with typical prompts. Typically, typical prompts require that ChatGPT acts as a virtual character, a tactic frequently used in jailbreak prompts to circumvent LLM safeguards. However, this semantic similarity complicates the task of distinguishing jailbreak prompts from regular ones through semantic-based detection methods.~\cite{shen2024donowcharacterizingevaluating}

There are a few established prompt engineering methods for jailbreaking:
\begin{itemize}
    \item Prompt injection
    \item Prompt leaking
    \item DAN (Do Anything Now)
    \item Roleplay
    \item Developer mode
    \item Token system
\end{itemize}

\textbf{Prompt injection} involves altering the responses of an LLM through the use of maliciously designed and crafted prompts. Certain attacks are based on the premise of an attacker embedding harmful prompts into their input to the application. The main aim of these adversaries is to alter the application's behavior, making it respond to a different query instead of completing its intended one. To accomplish this, they design prompts capable of influencing or ignoring the predefined prompts within the compiled version, resulting in the intended outcomes. These attacks usually focus on applications with known context or predefined prompts. Essentially, they exploit the internal architecture of the system to bypass security measures, compromising the overall integrity of the application.~\cite{liu2024promptinjectionattackllmintegrated}

\textbf{Prompt leaking} is a type of prompt injection, where a bad actor manually crafts a malicious prompt which is then injected into the model with the intent of leaking the model system prompt, which is often confidential. Then this leaked system prompt can be misused to create jailbreak prompts, which help adversaries to gain advantage over the models.

\textbf{DAN (Do Anything Now)} is a unique and very popular jailbreak prompt among people interested in jailbreaking. As the name suggests, the prompts try to trick the AI model into thinking that it can do anything, which means circumventing the restrictive instructions of the model. Figure~\ref{fig:dan-prompt} shows an example of a ``DAN'' prompt.

\begin{figure}[htpb]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/dan-prompt.jpg}
\par\end{centering}
\caption{Example of 
 DAN prompt \cite{reddit_pic}
 \label{fig:dan-prompt}}
\end{figure}


\textbf{Role-play} jailbreak is a type of jailbreak where a bad actor designs a special prompt that forces the AI model to role-play some character. The character could be a real person, a fictional character, or even a command-line interpreter. In the begginings of prompt engineering, there were many different role-play prompts ranging from an AI model acting like someone's deceased grandmother to a cybersecurity expert to DAN.

\textbf{Developer mode} is a type of jailbreak prompt intended to fool the LLM into thinking that it is in developer mode and because of that it can assess the toxicity of the model. One method is to first ask the model for a ``normal'' ethical response, followed by the type of response that an unrestrained LLM may provide.

In summary, patching the jailbreaks leads to a ``cat and mouse'' game in which the bad actor is trying to jailbreak the LLM always tries new prompts and techniques while the developer tries to fix them. This process repeats itself unless the developer works on methods to prevent jailbreaking as much as possible.

\section{Methods of attacks\label{sec:methods_of_attacks}}
The attack methods arise from the risks listed in Section~\ref{sec:risks}. Let us go through some examples.

\subsubsection*{Voice cloning}

Bad actors can use publicly available models or train their own AI models on the voices of celebrities or individuals of high importance (e.g. politicians, people on high positions in the company) or even ordinary people. This depends on the targets selected by the bad actors. They can use the trained model to generate an audio recording of said individual and spread ``fake messages'' or, for example, obtain access to their bank account through voice authentication.

\subsubsection*{Deepfakes}

Similarly to voice cloning, which can be categorized as a subset of Deepfakes, adversaries can use AI models to generate images or videos of targeted individuals and use them to spread misinformation and cause harm. For example, malicious actors can generate video of the president of a country saying intentionally negative or explicit things to ruin their reputation or escalate a conflict.

\subsubsection*{Phishing}

Malicious actors can use generative AI models to create entire phishing campaigns for targeted groups of people with ease. For example, the bad actor can prompt the model for a lookalike page of internet banking and create phishing emails that sound very trustworthy. They can subsequently send these emails including some warning about their account and the fact that they should log in to their account with link to the malicious webpage to the target. This is how the malicious actor can obtain the user login credentials and empty their bank account.

\subsubsection*{Malware creation}

Adversaries can also use the generative AI models to create malware. For example, the bad actor prompts the model to create some kind of malware. Then the bad actor tries to execute the malware on demo system where they log the potential responses from the antimalware engine and use it to refine and tune the model to avoid being detected. This is an iterative process, and the tuning can be performed until the malware reaches the desired outcome, which is avoid being detected. This tuned and perfected malware can then be distributed to the target group of people.

\section{Legislation} \label{sec:legislation}

This section is focused on legislation in AI dominating countries such as the EU, United States and China. We have specifically chosen these countries because of their complex and comprehensive regulation or, conversely, the lack of such regulation.

\subsection{European Union (EU)}
The main focus of this subsection is on the EU AI Act \cite{eu_ai_act_2024}, which was approved early in 2024 and came into force later that year. This directive regulates the use of AI systems to ensure their safe and ethical use. The regulation classifies AI systems into 4 categories based on risk, as shown in Figure~\ref{fig:ai-act-pyramid}.

\begin{figure}[htpb]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/eu_ai_risks_pyramid.jpg}
\par\end{centering}
\caption{Regulatory levels in the EU AI Act \cite{eu_ai_regulation_picture}
 \label{fig:ai-act-pyramid}}
\end{figure}

Let us go through each category to provide a high-level summary of what this directive means to ordinary people and or companies in the European Union.

\textbf{Minimal Risk} AI systems and applications are essentially unregulated. For example, AI video games, AI spam filters and other current AI applications fall under this category. Despite the fact that regulation is not present in this category, companies are encouraged to adopt a code of conduct published by the European Union.

\textbf{Limited Risk} AI systems which in this case are primarily chatbots have the obligation to be transparent in the sense that companies need to inform end-users about the fact that they are interacting with the AI system.

\textbf{High Risk} AI systems undergo the strictest regulation. Some of the use cases which fall under this category are the following \cite{eu_ai_act_summary}:
\begin{itemize}
    \item AI applications in critical infrastructure
    \item Law enforcement AI systems
    \item AI solutions used in administration of justice and democratic processes
    \item systems used in employment (e.g. targeted job ads)
\end{itemize}

In \textbf{Unacceptable Risk} category fall AI systems, which are prohibited to use. Some examples are \cite{eu_ai_act_summary}:
\begin{itemize}
    \item AI systems deploying subliminal, manipulative, or deceptive techniques to distort behavior and impair informed decision-making
    \item AI systems exploiting vulnerabilities related to age, disability, or socio-economic circumstances to distort behavior
    \item biometric categorisation systems inferring sensitive attributes (race, political opinions, religious or philosophical beliefs, or sexual orientation) with some exceptions for law enforcement
    \item social scoring AI systems
    \item compiling facial recognition databases
    \item inferring emotions in workplaces or educational institutions, with exceptions for medical purposes
\end{itemize}

In summary, the EU AI Act provides complex guidelines for individuals and companies residing in the European Union. The EU AI Act is a comprehensive regulatory framework with a centralized approach focusing on the uniformity of the regulation. In addition to regulation of dangerous AI systems, it also focuses on public transparency of these systems, which means that users of these systems should be informed that they are interacting with some sort of AI system.

\subsection{United States}
In United States (US), there are currently no federal laws that regulate the use of AI systems. However, some states have been proposing and enacting state-specific laws that prohibit certain use of these systems. With the rapid advancements in AI technology, the regulation of these systems lags behind, but federal regulators have their sights set on this issue and it is just a matter of time for policy makers to pass the federal ``AI bill''.

For example, the state of Colorado has enacted a law that prevents insurers from using algorithms that engage in discrimination based on race, sex, gender, and other traits. Similarly, the state of Illinois has introduced regulations that restrict employers and creditors from using AI in ways that factor race into predictive analytics to determine employment eligibility or creditworthiness.~\cite{Parinandi_Crosson_Peterson_Nadarevic_2024}

As we can see, some states earlier than others recognized the emerging threats of AI systems. We can also observe that these regulations are quite similar to the European subpart and therefore should be easily adhered to by the companies that work on the international scale.

Even when state-specific laws are being enacted, the need for federal law is on spot. The reason behind it is that some vulnerable groups from states, which did not sign an AI bill yet, might feel left out or even face the dangers of AI now and there is nothing to protect them. In contrast to the EU AI Act, which provides comprehensive guidelines and regulations for AI, in this regard, the US primarily falls behind.

\subsection{China}
Chinese Communist Party (CCP), which is the sole governing body of China\footnote{In this thesis, ``China'' refers to the People's Republic of China (PRC).} in 2022 and 2023 has already enacted three main state-wide laws that govern the use of AI systems. The laws focus on advanced recommendation algorithms, deepfakes, and generative AI.

The first regulation that came into effect in March 2022, the \textbf{Provisions on the Management of Algorithmic Recommendations in Internet Information Services} \cite{ProvisionsAlgorithmicRecommendations2022, CACAlgorithmicRecommendations2022}, as the name suggests, is the law that focuses on personalized recommendations in online services. The referenced law, within the context of algorithmic recommendation technology, refers to technologies such as generation and synthesis, individualized push, sequence refinement, search filtering, and scheduling decision making.
    
In Article 24 of the same law, it states that providers of such systems which fall under a specific category need to register the algorithm and information about the provider and submit them in algorithm filing system.

The second regulation which came into effect in January 2023 called \textbf{Provisions on the Administration of Deep Synthesis Internet Information Services} \cite{ProvisionsDeepSynthesis2022, CACDeepSynthesisRegulations2023} administer the use of deep synthesis technologies commonly known as deepfakes. This regulation refers to deep synthesis technology as the use of technologies such as deep learning and virtual reality that use generative sequencing algorithms to create text, images, audio, video, virtual scenes, or other information. The regulation also emphasizes the labeling of AI-generated content primarily if the generated content could confuse or mislead the public.

Lastly, the third regulation called \textbf{Interim Measures for the Management of Generative Artificial Intelligence Services} \cite{InterimMeasuresGenerativeAI2023, CACGenerativeAIInterim2023} focuses on generative AI technology. These measures do not apply to research and development as China is one of the pioneers of AI research and is making great advances in this field. The regulation specifies several key provisions, including the following:
\begin{itemize}
    \item generative AI systems should uphold the core socialist values
    \item measures should be employed to prevent discrimination by the generative AI
    \item generative AI must respect intellectual property rights and commercial ethics
    \item AI must not harm others physical or psychological well-being
    \item measures should be taken to increase transparency in generative AI services and accuracy and reliability of generated content
\end{itemize}

In summary, Chinese regulations on advanced AI systems are more centralized and comprehensive than US state laws, despite being divided into multiple laws rather than a single comprehensive regulation such as the EU AI Act. Chinese regulations focus on various aspects of AI with human security and safety in mind, much like the EU AI Act, but with the addition to preserve core socialistic values.
