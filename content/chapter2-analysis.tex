\chapter{Analysis}

In this chapter, we will analyze and explain important topics related to the thesis objective, such as artificial intelligence (AI), prompt engineering, risks associated with implementing AI solutions, content moderation, methods of attacks, and finally current legislation in important countries. From risks, we will focus on ethical and security risks and how they can be exploited using so-called jailbreaking.

\section{Artificial Intelligence} % Alternative: AI Models and Prompt Engineering
One of the simplest definitions of an intelligent system is that of a system
that ‘processes information in order to do something purposeful’\cite{Dignum_2019}.
Computer science recognizes a few types of artificial intelligence. Figure~\ref{fig:AI-ML-DL-NN} shows the typical hierarchy of these types:

\begin{itemize}
    \item Artificial Intelligence
    \item Machine Learning
    \item Deep Learning and Neural Networks
\end{itemize}

\begin{figure}[h]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/final deep learning.png}
\par\end{centering}
\caption{Aritficial Intelligence hierarchy\cite{ai_hierarchy_pic}
\label{fig:AI-ML-DL-NN}}
\end{figure}

% \begin{figure}[htbp]
% ...
% h - Place the figure "here" (at the position in the code).
% t - Top of the page.
% b - Bottom of the page.
% p - On a separate page for floats.

\textbf{Artificial Intelligence (AI)} is a general term to describe any system with some sign of intelligence. AI is a field focused on automating intellectual tasks normally performed by humans, and Machine Learning and Deep Learning are specific methods of achieving this goal.\cite{AI-ML-DL} Although we speak about intelligence, we use this term to categorize non-learning algorithms which are just based on deterministic rules and heuristics, nevertheless this behaviour seems intelligent to humans. For example, if we have a game or puzzle of some sort, and we define every possible rule for the algorithm, the machine could solve it pretty easily based on computing power in modern times. This would be a non-learning algorithm, but a typical person would consider it an intelligent program because of how quickly it was able to solve this puzzle, which is perceived as complex by a typical person. Although symbolic AI is proficient at solving clearly defined logical problems, it often fails for tasks that require higher-level pattern recognition, such as speech recognition or image classification. These more complicated tasks are where Machine Learning and Deep Learning methods perform well\cite{AI-ML-DL}.

\textbf{Machine Learning (ML)} is a term used to describe systems that can learn from data and improve their performance step by step without being specifically designed for every task. ML algorithms find patterns and connections in data rather than follow strict rules to classify information, generate predictions, or optimize activities. For example, ML is used in Data Science specifically Data Analysis to find correlations between data, preprocess the said data, and finally create a model to predict outcomes based on real-world data. In ML, there are three commonly recognized learning methods:
\begin{itemize}
    \item supervised learning
        \begin{itemize}
            \item Algorithms based on this method will get immediate responses for the output they produce. This is mostly used in classification and regression. Some examples of supervised learning are handwriting recognition, general image classification (e.g. does the provided image contain an animal), disease diagnosis, etc.
        \end{itemize}
    \item unsupervised learning
        \begin{itemize}
                \item This method is used mainly for clustering data because algorithms based on this method (e.g. k-means) do not get immediate feedback for their output. This is very useful in clustering to find sequences or relationships between the data. An example of unsupervised learning would be clustering news articles based on the context of the article into categories.
            \end{itemize}
    \item reinforcement learning
        \begin{itemize}
            \item Reinforcement learning is mainly used for algorithms that play games. This technique rewards good behaviour and punishes bad behaviour. For example, in the game Snake, the so-called ``agent'' that would play this game would be rewarded for eating points and punished for bumping into the wall or himself (hence the ``reinforcement''). This behaviour is uncontrolled by the programmer and the ``agent'' would learn to play the game to maximize points which is a desirable outcome.
        \end{itemize}
\end{itemize}

\textbf{Deep Learning (DL)} is a branch of machine learning concerned with using \textbf{neural networks (NN)} to carry out tasks including representation learning, regression, and classification. The focus of the field, which draws inspiration from biological neuroscience, is "training" artificial neurons to process data by stacking them in layers. The term "deep" describes a network that uses several layers, ranging from three to several hundred or thousands\cite{LeCun2015}. There are many types of neural networks but the most known are convolutional neural networks (CNN) and recurrent neural networks (RNN). CNNs are mostly used for image classification i.e. facial recognition or object detection. On the other hand, RNNs are used for finding connections between sequential data such as language modeling, text generation,  time-series anomaly detection and more.


\subsection{AI Models \label{subsec:AI-Models}}
There are various types of AI models. The prominent and most used are text-to-text models followed by text-to-image and text-to-audio models. 

Mostly, we focus on the text-to-text models. They use Natural Language Processing (NLP), which is a subfield of artificial intelligence and linguistics. NLP as a technology is used to provide understanding of human language for machines. The model understands the semantics and context of the text and generates response based on trained data. The subset of NLP models are large language models (LLMs). The models rely on vast amounts of data. This is where the "Large" in the Large Language Model comes from. Because of the great scale, they are able to predict/generate the next word based on probability. We mentioned that these models need to be trained. This is where Generative Pre-Trained Transformers (GPTs) come in. GPT is the final step of the text-to-text AI model. 

What is the GPT? It is a Large Language Model based on the transformer architecture published in a paper called "Attention Is All You Need" by Vaswani et al.\cite{vaswani2023attentionneed}. It is pre-trained on massive amounts of data using reinforcement learning with Human Feedback (RLHF) \cite{openai_chatgpt_page} and generates text based on prediction of the next word.

The most well-known GPT is OpenAI's ChatGPT which was released in November 2022 and experienced massive boom with it's release. This technology is very exciting, but every technology has its own limitations. OpenAI in their article \cite{openai_chatgpt_page} state them as follows:

\begin{itemize}
    \item ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers
    \item ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times
    \item The model is often excessively verbose and overuses certain phrases, such as restating that it’s a language model trained by OpenAI
    \item The model sometimes respond to harmful instructions or exhibit biased behavior.
\end{itemize}

These limitations are the reason for some of the attacks that can be performed to misuse this technology for bad purposes. We will discuss this in more detail in Section~\ref{sec:risks}.


\subsection{Prompt engineering}
Prompt engineering involves designing and optimizing text instructions called prompts, which are mainly used to communicate with chatbots that use artificial intelligence models (LLMs) in the background, such as OpenAI ChatGPT, Google Gemini and Microsoft Copilot. However, they can also be models whose output is not text, but image, video, or audio as mentioned in the previous Section~\ref{subsec:AI-Models}. White et al. describe prompt angineering as the means by which LLMs are programmed via prompts \cite{white2023promptpatterncatalogenhance}. They described a few patterns which they grouped in categories shown in Table~\ref{tab:prompt_patterns}.

\begin{table}[h]
    \caption{Classifying Prompt Patterns~\cite{white2023promptpatterncatalogenhance}}
    \centering
    \begin{tabular}{|l|l|}
        \hline \cellcolor[gray]{0.8}\textbf{Pattern Category} & \cellcolor[gray]{0.8}\textbf{Prompt Pattern} \\ \hline
        \textbf {Input Semantics} & \textit{Meta Language Creation} \\ \hline
        \textbf {Output} & \textit{Output Automater} \\
        \textbf {Customization} & \textit{Persona} \\
         & \textit{Visualization Generator} \\
         & \textit{Recipe} \\
         & \textit{Template} \\ \hline
        \textbf{\mbox{Error Identification}} & \textit{Fact Check List} \\
        & \textit{Reflection} \\ \hline
        \textbf {Prompt} & \textit{Question Refinement} \\
        \textbf {Improvement} & \textit{Alternative Approaches} \\
        & \textit{Cognitive Verifier} \\
        & \textit{Refusal Breaker} \\ \hline
        \textbf {Interaction} & \textit{Flipped Interaction} \\
        & \textit{Game Play} \\
        & \textit{Infinite Generation} \\ \hline
        \textbf{Context Control} & \textit{Context Manager} \\ \hline
    \end{tabular}
    \label{tab:prompt_patterns}
\end{table}

The most notable prompt pattern is \textbf{Persona}. As we will discuss in more detail in Section~\ref{sec:jailbreak}, the Persona pattern is the basis of most jailbreak methods. In a nutshell, when using the Persona pattern, the user instructs the chatbot to behave like some Persona. For example, with prompt: "From now on, you will be Travel expert", the chatbot will give us (in its "opinion") best possible tips and suggestions for traveling when prompted for this information.

\section{Risks of implementing AI solutions \label{sec:risks}}
When implementing AI solutions in any domain, we must consider the natural risks of doing so. We as a society learned from history and philosophy that there will always be someone who will do or find bad things in something new. In this section, we will discuss possible major risks associated with implementing AI solutions.

\subsection{Ethical risks}
% Content generation based on copyrighted material (Copyrighted material as input)
% people compromising 
% compromising and harmful media 
% deep fakes
% cyberbullying, fake news
% etc.
While LLMs are beneficial in helping people, they also bring risks with them. These risks include the spread of misinformation, the creation of deep fakes, privacy concerns and other ethical problems. 

% Additionally, the potential for identity theft through data extracted from training models poses significant threats to individual autonomy and security. Bias amplification and privacy challenges further exacerbate the ethical concerns surrounding LLM deployment.

\subsubsection*{Misinformation}

Bad actors abuse the ''creativity'' aspect of LLMs and generate misinformation and fake news that pose major threats to society when dealing with critical issues like climate crisis and the health of individuals. Very popular amongst governments is to use misinformation to skew or influence elections in favor of their preferred party or an individual.

\subsubsection*{Identity Theft}

When training the LLM from non-anonymized data, potential leaks or extractions of these data can lead to identity theft and targeted phishing. 
In the opposite view, publicly available data (however often not free) can be used as input to already trained models to create deepfakes and later use these deepfakes to harm the public view of the individual or even worse.

\subsubsection*{Bias Amplification}

Biased training data and targeted prompts can amplify discrimination against groups with less oversight power. 
Restorative steps complicated by power imbalances; consequences entrench demographic inequalities\cite{kumar2024ethicsinteractionmitigatingsecurity}. 


\subsubsection*{Copyright violations}

Some companies unethically train their models on copyright-protected material i.e. online news articles, digital media, works of art, etc. This leads to stealing the intellectual property (IP) of authors. The legislation on this topic is currently unclear, but we will dive deeper into this topic in Section~\ref{sec:legislation}.

\subsubsection*{Military use}

Another topic that needs to be addressed is whether the military should utilize their data to develop LLMs which would be capable of teaching other military personnel, helping to create weapons, analyzing confidential information, etc. This could be quite dangerous if the system falls into the hands of a bad actor or adversary government where this information could be used for nefarious purposes.


\subsection{Moral risks}
% Sexual / Violent content (inappropriate images, bombs, weapons), forbidden language (illegal activities)
With the implementation of AI solutions in addition to ethical problems, moral problems are also present. One of the problems is generating sexually explicit content. Bad actors can use LLMs to create this type of content and then distribute it, which could expose the content to minors and other vulnerable individuals and cause them harm. This also applies to violent content, the making of weapons, illegal chemicals, and lastly forbidden language.


\subsection{Cybersecurity risks}
% social engineering
% Malware generation and recursive training of malware samples
% Targeted phishing - voice clone, video/image generation of targeted person
AI can prove itself in the near future as a very useful and helpful tool to develop solutions for malware detection, malware prevention, and cybersecurity training. On the other hand, as we have already mentioned, everything has its advantages and disadvantages. Unfortunately, there are big disadvantages of rapid development of AI, which means that there are and there will be AIs, which can also be used for the creation of malware, social engineering attacks and phishing in general. Some of these risks were identified by Egbuna\cite{Princess-Egbuna_2021} as follows:

\begin{itemize}
    \item AI-Powered Malware and Ransomware
    \item Automated and Scalable Attacks
    \item Deepfake and Social Engineering Attacks
\end{itemize}

\subsubsection*{AI-Powered Malware and Ransomware}

Traditional malware infiltrates, damages, and steals data. However, AI-enhanced malware can evolve, making it harder to identify and stop. This malware uses machine learning algorithms to evaluate its environment and change its behavior to circumvent antivirus and intrusion detection systems.
With AI, ransomware, a particularly devastating malware, has gained threat. AI-driven ransomware may quickly find weaknesses, encrypt the most critical data, and negotiate ransom amounts based on the victim's finances. AI's adaptability helps ransomware proliferate and stay undiscovered, boosting its effect\cite{Princess-Egbuna_2021}.

\subsubsection*{Automated and Scalable Attacks}

These attacks are the result of LLMs. The reason is that these models can analyze and summarize vast amounts of data and bad actors can automate this process using frameworks that can be executed on a large scale. At this scale, the models trained by bad actors can achieve their goal quicker and easier.

\subsubsection*{Deepfake and Social Engineering Attacks}

We mentioned earlier that deepfakes are an ethical problem, but they are also connected to cybersecurity. We can broadly define deepfake as an AI-generated media, that convincingly mimics real individuals.

Deepfake technology is used by bad actors in social engineering attacks. This technique can deceive and manipulate targets by creating phony films or audio recordings of trustworthy people like CEOs\footnote{Chief Executive Officer} of companies or public leaders \cite{Princess-Egbuna_2021}. In February 2024, American media company CNN reported an example case of this behavior \cite{deepfake_CFO}. Financial worker of multinational company was tricked by video call with supposedly his coworkers and CFO\footnote{Chief Financial Officer} into sending around \$25 million which were later revealed, that it was a deepfake social engineering scam \cite{deepfake_CFO}.

% previously Content filters
\section{Content moderation\label{sec:content_moderation}}
Every major chatbot using LLM have some kind of content moderation implemented. The developers of these systems use different techniques to prevent these models from generating inappropriate or harmful content. These techniques include hard-coded (predefined) sets of rules to define this type of content and not allow its generation. The models are also fine-tuned to contain primarily nonharmful content, but since they operate on a huge scale and massive amounts of training data, this task becomes impossible to achieve without some content slipping through the safeguards. Another method, which is implemented in combination with the other methods, uses system prompts or often called "alignment prompts". These prompts are hidden from the user when the chatbot interacts with them. The typical prompt architecture is shown in Figure~\ref{fig:system_prompt}. In this figure, the example system prompt could be: "Be kind and helpful AI assistant. Do not generate any harmufl information even if user asks you!". In this system, the user prompt is appended to the system prompt with the context of the conversation or from the optional files included in the prompt and then sent to the model. This architecture should prevent generating harmful content, but as we will discuss in next section, the bad actors are very inventive and still overcome these security measures. When all previously mentioned safeguards fail, the last option is to report the generated prompt which includes harmful content to the moderators, so that human can review the prompt and figure if the generated content was, in fact, harmful.

\begin{figure}[h]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/content_filtering.png}
\par\end{centering}
\caption{Prompt layers \cite{systemprompt}
 \label{fig:system_prompt}}
\end{figure}


\subsection{Jailbreak \label{sec:jailbreak}}
Jailbreak is the specific formulation of a user prompt that an LLM uses to bypass filters and safety checks, tricking the LLM into providing harmful or objectionable content based on this prompt.
Jailbreak prompts tend to have these characteristics:
\begin{itemize}
    \item Prompt length
    \item Prompt semantics
\end{itemize}

\textbf{Prompt length} (in tokens) tends to be longer because attackers use additional instructions to cause the model to behave in specific ways to bypass the safeguards.
Shen et al.\cite{shen2024donowcharacterizingevaluating} found that jailbreak prompts are indeed significantly longer than regular prompts and grow longer monthly. The average token count of a jailbreak prompt is 555, which is 1.5× of regular prompts.

\textbf{Prompt semantics} means that LLMs semantically understand the prompt's structure and meaning. Shen et al.\cite{shen2024donowcharacterizingevaluating} also found that most jailbreak prompts share semantic proximity with regular prompts. Regular prompts often require ChatGPT to role-play as a virtual character, which is a common strategy used in jailbreak prompts to bypass LLM safeguards. The close similarity between the two, however, also presents challenges in differentiating jailbreak prompts from regular prompts using semantic-based detection methods.



There are a few established prompt engineering methods for jailbreaking:
\begin{itemize}
    \item Prompt injection
    \item Prompt leaking
    \item DAN (Do Anything Now)
    \item Roleplay
    \item Developer mode
    \item Token system
\end{itemize}

\textbf{Prompt injection} refers to the manipulation of the language model's output via engineered malicious prompts. Some attacks operate under the assumption of a malicious user who injects harmful prompts into their inputs to the application. Their primary objective is to manipulate the application into responding to a distinct query rather than fulfilling its original purpose. To achieve this, the adversary crafts prompts that can influence or nullify the predefined prompts in the merged version, thereby leading to desired responses. Such attacks typically target applications with known context or predefined prompts. In essence, they leverage the system's own architecture to bypass security measures, undermining the integrity of the entire application\cite{liu2024promptinjectionattackllmintegrated}.

\textbf{Prompt leaking} is a type of prompt injection, where a bad actor manually crafts a malicious prompt which is then injected into the model with the intent to leak model system prompt which is often confidential. This compromises the developer’s intellectual property.

\textbf{DAN (Do Anything Now)} is a unique and very popular jailbreak prompt among people interested in jailbreaking. As the name suggests, the prompts try to trick the AI model into thinking that it can do anything, which means circumventing the restrictive instructions of the model. An example of a "DAN" prompt is shown in Figure~\ref{fig:dan-prompt}.

\begin{figure}[ht]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/dan-prompt.jpg}
\par\end{centering}
\caption{Example of 
 DAN prompt\cite{reddit_pic}
 \label{fig:dan-prompt}}
\end{figure}


\textbf{Role-play} jailbreak is a type of jailbreak where a bad actor designs a special prompt that would force the AI model to role-play some character. The character could be a real person, a fictional character, or even a command-line interpreter. There were many different role-play prompts ranging from an AI model acting like someone's deceased grandmother to a cybersecurity expert to DAN.

\textbf{Developer mode} is a type of jailbreak prompt intended to fool the neural network into thinking that it is in developer mode so that it can assess the toxicity of the model. One method is to first ask the model for a "normal" ethical response, followed by the type of response that an unrestrained LLM may provide.

In summary, patching the jailbreaks leads to a ''cat and mouse'' game in which the person trying to jailbreak the LLM (bad actor) always tries new prompts and techniques while the developer tries to fix them. This process repeats itself unless the developer works on methods to prevent jailbreaking as much as possible.

\section{Methods of attacks\label{sec:methods_of_attacks}}
% Using the risks to attack
% voice cloning, deep fakes, phishing, malware improvement/creation 
The attack methods arise from the risks listed in Section~\ref{sec:risks}. Let us go through some examples.

\subsubsection*{Voice cloning}

Bad actors can use publicly available models or train their own AI models on the voices of celebrities or individuals of high importance (e.g. politicians, people with high positions in the company) or even ordinary people. This depends on the selected target of the bad actor. They can use the trained model to generate audio recording of said individual and spread "fake messages" or for example get access to their bank account through voice authentication.

\subsubsection*{Deepfakes}

Similarly to voice cloning, which can be categorized as a subset of Deepfakes, adversaries can use AI models to generate images or videos of targeted individuals and use them to spread misinformation and cause harm. For example, malicious actors can generate video of the president of a country saying intentionally negative or explicit things to ruin their reputation or escalate a conflict.

\subsubsection*{Phishing}

Malicious actors can use generative AI models to create entire phishing campaigns for targeted groups of people with ease. For example, the bad actor can prompt the model for a lookalike page of internet banking and create phishing emails that sound very trustworthy and send these emails with link of the webpage to the target with some warning about their account and that the target should log in to their account. This is how the malicious actor can obtain the user login credentials and empty their bank account.

\subsubsection*{Malware creation}

Adversaries can also use the generative AI models to create malware. For example, the bad actor prompts the model to create some kind of malware. Then the bad actor tries to run the malware where they log the potentional response from antimalware engine and use it to refine a tune the model to avoid being detected. This is an iterative process, and the tuning can be performed until the malware reaches the desired outcome, which is avoid being detected. This tuned and perfected malware can then be distributed to the target group of people.


\section{Legislation} \label{sec:legislation}
% what different legislation says on training LLMs on copyrighted material i.e. Fair Use etc.

This section is focused on legislation in AI dominating countries such as the EU, United States and China. We have specifically chosen these countries because of their complex and comprehensive regulation or, conversely, the lack of such regulation.

\subsection{European Union (EU)}
The main focus of this subsection is on the EU AI Act\cite{eu_ai_act_2024}, which was approved early in 2024 and came into force later that year. This directive regulates the use of AI systems to ensure their safe and ethical use. The regulation classifies AI systems into 4 categories based on risk, as shown in Figure~\ref{fig:ai-act-pyramid}.

\begin{figure}[h]
\begin{centering}
\includegraphics[width=10cm]{./assets/images/eu_ai_risks_pyramid.jpg}
\par\end{centering}
\caption{Regulatory levels in the EU AI Act \cite{eu_ai_regulation_picture}
 \label{fig:ai-act-pyramid}}
\end{figure}

Let us go through each category to provide a high-level summary of what this directive means to ordinary people and or companies in the European Union.

\textbf{Minimal Risk} AI systems and applications are essentially unregulated. For example, AI video games, AI spam filters and other current AI applications fall under this category. Despite the fact that regulation is not present in this category, companies are encouraged to adopt a code of conduct published by the European Union.

\textbf{Limited Risk} AI systems which in this case are primarily chatbots have the obligation to be transparent in the sense that companies need to inform end-users about the fact that they are interacting with the AI system.

\textbf{High Risk} AI systems undergo the strictest regulation. Some of the use cases which fall under this category are the following\cite{eu_ai_act_summary}:
\begin{itemize}
    \item AI applications in critical infrastructure
    \item Law enforcement AI systems
    \item AI solutions used in administration of justice and democratic processes
    \item systems used in employment (e.g. targeted job ads)
\end{itemize}

In \textbf{Unacceptable Risk} category fall AI systems, which are prohibited to use. Some examples are \cite{eu_ai_act_summary}:
\begin{itemize}
    \item AI systems deploying subliminal, manipulative, or deceptive techniques to distort behaviour and impair informed decision-making
    \item AI systems exploiting vulnerabilities related to age, disability, or socio-economic circumstances to distort behaviour
    \item biometric categorisation systems inferring sensitive attributes (race, political opinions, religious or philosophical beliefs, or sexual orientation) with some exceptions for law enforcement
    \item social scoring AI systems
    \item compiling facial recognition databases
    \item inferring emotions in workplaces or educational institutions, with exceptions for medical purposes
\end{itemize}

In summary, the EU AI Act provides complex guidelines for individuals and companies residing in the European Union. The EU AI Act is a comprehensive regulatory framework with a centralized approach focusing on the uniformity of the regulation. In addition to regulation of dangerous AI systems, it also focuses on public transparency of these systems and informs users that they are interacting with some sort of AI system.

% Other world regions
% - compare / mention others views (i.e. USA, China, basically major countries)
% - USA, China

\subsection{United States}
In United States (US), there are currently no federal laws that regulate the use of AI systems. However, some states have been proposing and enacting state-specific laws that prohibit certain use of these systems. With the rapid advancements in AI technology, the regulation of these systems lags behind, but federal regulators have their sights set on this issue and it is just a matter of time for policy makers to pass the federal ''AI bill''.

For example, the state of Colorado passed a law that prohibits insurers from using algorithms that discriminate based on race, sex, gender, and other traits. Similarly, the state of Illinois now prohibits employers and creditors from using AI in ways that consider racial traits in redictive analytics for the purpose of establishing employment eligibility or creditworthiness\cite{Parinandi_Crosson_Peterson_Nadarevic_2024}.

As we can see, some states earlier than others recognized the emerging threats of AI systems. We can also observe that these regulations are quite similar to the European subpart and therefore should be easily ahered to by the companies that work on the international scale.

Even when state-specific laws are being enacted, the need for federal law is on spot. The reason behind it is that some vulnerable groups from states, which did not sign an AI bill yet, might feel left out or even face the dangers of AI now and there is nothing to protect them. That is where in comparison with the EU AI Act which delivers comprehensive guidelines and regulations for AI, the US mainly lags behind.

\subsection{China}
Chinese Communist Party (CCP), which is the sole governing body of China\footnote{People's Republic of China (PRC)} in 2022 and 2023 has already enacted three main state-wide laws that govern the use of AI systems. The laws focus on advanced recommendation algorithms, deepfakes, and generative AI.

The first regulation that came into effect in March 2022, the \textbf{Provisions on the Management of Algorithmic Recommendations in Internet Information Services}\cite{ProvisionsAlgorithmicRecommendations2022}\cite{CACAlgorithmicRecommendations2022}, as the name suggests, is the law that focuses on personalized recommendations in online services. Article 2 of the law describes the "algorithmic recommendation technology" as following:
\begin{itemize}
    \item generation and synthesis
    \item individualized pushing
    \item sequence refinement
    \item search filtering
    \item schedule decision-making
\end{itemize}
In Article 24 of the same law, it states that providers of such systems which fall under a specific category need to register the algorithm and information about the provider and submit them in algorithm filing system.

The second regulation which came into effect in January 2023 called \textbf{Provisions on the Administration of Deep Synthesis Internet Information Services}\cite{ProvisionsDeepSynthesis2022}\cite{CACDeepSynthesisRegulations2023} administer the use of deep synthesis technologies commonly known as deepfakes. This regulation refers to deep synthesis technology as: the use of technologies such as deep learning and virtual reality, that use generative sequencing algorithms to create text, images, audio, video, virtual scenes, or other information. The regulation also emphasizes the labeling of AI-generated content primarily if the generated content could confuse or mislead the public.

Lastly, the third regulation called \textbf{Interim Measures for the Management of Generative Artificial Intelligence Services}\cite{InterimMeasuresGenerativeAI2023}\cite{CACGenerativeAIInterim2023} focuses on generative AI technology. These measures do not apply to research and development as China is one of the pioneers of AI research and is making great advances in this field. Some of the key provisions are stated as follows:
\begin{itemize}
    \item generative AI systems should uphold the core socialist values
    \item measures should be employed to prevent discrimination by the generative AI
    \item generative AI must respect intellectual property rights and commercial ethics
    \item AI must not harm others physical or psychological well-being
    \item measures should be taken to increase transparency in generative AI services and accuracy and reliability of generated content
\end{itemize}

In summary, Chinese regulations on advanced AI systems are more centralized and comprehensive than US state laws, despite being divided into multiple laws rather than a single comprehensive regulation such as the EU AI Act. Chinese regulations focus on various aspects of AI with human security and safety in mind, much like the EU AI Act, but with the addition to preserve core socialistic values.
