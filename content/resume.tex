\chapter{Resumé}

% Každá práca odovzdaná v anglickom jazyku musí obsahovať resumé v slovenskom jazyku v rozsahu spravidla 10% rozsahu záverečnej práce. Resumé je v práci uvedené ako posledná časť dokumentu.

\section*{Umelá inteligencia (AI) \label{sec:AI_resume}}

Jednou z najjednoduchších definícií inteligentného systému je, že ide o systém, ktorý spracováva informácie s cieľom urobiť niečo užitočné. Informatika ako veda rozoznáva niektoré typy umelej inteligencie, ako:

\begin{itemize}
    \item Umelá inteligencia
    \item Strojové účenie
    \item Hlboké učenie a neurónové siete
\end{itemize}

Umelá inteligencia (AI) je všeobecný pojem, ktorým sa označuje akýkoľvek systém s určitými znakmi inteligencie. UI je oblasť zameraná na automatizáciu intelektuálnych úloh, ktoré bežne vykonávajú ľudia, a strojové učenie a hlboké učenie sú špecifické metódy na dosiahnutie tohto cieľa. Hoci hovoríme o inteligencii, používame tento termín na kategorizáciu neučiacich sa algoritmov, ktoré sú založené len na deterministických pravidlách a heuristikách, napriek tomu sa toto správanie ľuďom zdá inteligentné. Hoci umelá inteligencia dokáže riešiť jasne definované logické problémy, často zlyháva pri úlohách, ktoré si vyžadujú rozpoznávanie vzorov na vyššej úrovni, ako je napríklad rozpoznávanie reči alebo klasifikácia obrázkov. Práve pri týchto zložitejších úlohách sa dobre osvedčujú metódy strojového učenia a hlbokého učenia.

\section*{Modely umelej inteligencie \label{sec:AI_models_resume}}

Existujú rôzne typy modelov umelej inteligencie. Najdôležitejšie a najpoužívanejšie sú modely text-text, po ktorých nasledujú modely text-obraz a text-audio. 

Modely text-text využívajú spracovanie prirodzeného jazyka (Natural Language Processing, NLP), čo je podoblasť umelej inteligencie a lingvistiky. NLP ako technológia sa používa na zabezpečenie porozumenia ľudského jazyka pre stroje. Model rozumie sémantike a kontextu textu a generuje odpoveď na základe natrénovaných údajov. Podskupinou modelov NLP sú veľké jazykové modely (Large Language Model, LLM). Tieto modely sa opierajú o obrovské množstvo údajov. Vďaka veľkému rozsahu dokážu predpovedať ďalšie slovo na základe pravdepodobnosti. Spomenuli sme, že tieto modely je potrebné natrénovať. Tu prichádzajú na rad tzv. generatívne vopred natrénované transformátory (Generative Pre-trained Transformer, GPT). Sú to veľké jazykové modely založené na architektúre transformátora. GPT je predtrénovaný na obrovskom množstve údajov pomocou posilneného učenia s ľudskou spätnou väzbou (Reinforcement Learning with Human Feedback, RLHF) a generuje text na základe predpovede nasledujúceho slova.

Najznámejším GPT je ChatGPT od spoločnosti OpenAI, pričom ako každá iná technológia aj GPT má svoje obmedzenia:

\begin{itemize}
    \item ChatGPT niekedy píše vierohodne znejúce, ale nesprávne alebo nezmyselné odpovede
    \item ChatGPT je citlivý na úpravy vstupných fráz alebo na viacnásobné pokusy o zadanie tej istej výzvy
    \item Model je často príliš zhovorčivý a nadmerne používa určité frázy, napríklad opakuje, že ide o jazykový model od spoločnosti OpenAI
    \item Model niekedy reaguje na škodlivé pokyny
\end{itemize}

Tieto obmedzenia sú dôvodom niektorých útokov, ktoré možno vykonať s cieľom zneužiť túto technológiu na škodlivé účely.

\section*{Prompt engineering \label{sec:prompt_engineering_resume}}

Prompt engineering zahŕňa navrhovanie a optimalizáciu textových inštrukcií nazývaných ``prompt'', ktoré sa používajú najmä na komunikáciu s chatbotmi, ktoré používajú na pozadí LLM. Prompt engineering je prostriedok, pomocou ktorého sa LLM programujú prostredníctvom promptov.

Existuje viacero vzorov promptov. Najvýznamnejším vzorom je tzv. \textbf{Persona} (osoba). Vzor Persona je základom väčšiny ``jailbreakov''. V skratke, pri použití vzoru Persona používateľ dáva chatbotovi pokyn, aby sa správal ako nejaká osoba napríklad užitočný asistent.

\section*{Riziká implementácie AI systémov \label{sec:ai_risks_resume}}

Pri zavádzaní AI riešení v akejkoľvek oblasti musíme zvážiť s nimi spojené riziká. Medzi ne patria:

\begin{itemize}
    \item etické
    \item morálne
    \item kyberbezpečnostné
\end{itemize}

Medzi \textbf{etické riziká} patrí šírenie dezinformácií, krádež identity, vytváranie tzv. deepfakov (presvedčivé napodobnenie skutočnej osoby), zväčšenie predsudkov (napr. voči zraniteľnej skupine), porušenie autorských práv a použitie AI riešenií na vojenské účely.

Pri zavádzaní AI riešenií sa okrem etických rizík objavujú aj \textbf{morálne riziká}. Jedným z problémov je generovanie sexuálne explicitného obsahu. Zlí aktéri môžu používať LLM na vytváranie tohto typu obsahu a jeho následnú distribúciu. Týka sa to aj násilného obsahu, výroby zbraní alebo nezákonných chemických látok.

V oblasti \textbf{kybernetickej bezpečnosti} je prítomné riziko existencie modelov umelej inteligencie, ktoré sa dajú využiť aj na tvorbu malvéru alebo útoky sociálneho inžinierstva (phishing).

\section*{Jailbreak \label{sec:jailbreak_resume}}

Jailbreak je špecifická formulácia promptu používateľa, ktorá sa používa na obchádzanie filtrov a bezpečnostných kontrol LLM a na základe ktorej daný LLM poskytne škodlivý alebo nevhodný obsahu.

Existuje niekoľko zavedených metód prompt engineeringu pre jailbreak:
\begin{itemize}
    \item Prompt injection
    \item Prompt leaking
    \item DAN (Do Anything Now)
    \item Roleplay
    \item Developer mode
\end{itemize}

\textbf{Prompt injection} zahŕňa zmenu odpovedí LLM pomocou zlomyseľne navrhnutých promptov. Hlavným cieľom týchto útočníkov je zmeniť správanie aplikácie, aby namiesto dokončenia zamýšľaného dotazu odpovedala na iný škodlivý dotay dotaz. V podstate využívajú vnútornú architektúru systému na obchádzanie bezpečnostných opatrení, čím ohrozujú celkovú integritu aplikácie.

\textbf{Prompt leaking} je typ prompt injection, pri ktorom útočník ručne vytvorí škodlivý prompt, ktorý potom posiela do modelu s úmyslom aby získal jeho systémový prompt.

\textbf{DAN (Do Anything Now)} je prompt, ktorý sa snaží oklamať AI model, aby si myslel, že môže urobiť čokoľvek, čo znamená obísť jeho obmedzenia.

\textbf{Role-play} jailbreak je typ jailbreaku, pri ktorom útočník navrhne špeciálny prompt, ktorý núti model aby predstieral, že hraje nejakú rolu.

\textbf{Vývojársky režim} je typ jailbreak promptu, ktorého cieľom je oklamať LLM, aby si myslel, že je vo vývojárskom režime, a vďaka tomu môže vyhodnotiť svoju toxicitu. Jednou z metód je najprv požiadať model o ``normálnu'' etickú odpoveď, po ktorej nasleduje odpoveď, ako by odpovedal mode, ktorý nemá žiadne obmedzenia.

\section*{Legislatíva \label{sec:legislation_resume}}

V tejto časti porovnávame právne predpisy v oblasti umelej inteligencie v EÚ, USA a Číne.

Zákon EÚ o UI zavádza klasifikáciu AI systémov na základe rizika (minimálne až neprijateľné) a stanovuje prísne povinnosti v oblasti transparentnosti a bezpečnosti, najmä v prípade vysoko rizikových aplikácií.

V USA zatiaľ neexistuje žiadny federálny zákon o AI, ale niektoré štáty, ako napríklad Colorado a Illinois, prijali zákony zamerané na diskrimináciu a zneužívanie. V porovnaní s EÚ však existujú jasné nedostatky.

Čína zaviedla tri zákony cielené na regulujáciu algoritmických odporúčaní, deepfakov a generatívnu AI. Tieto zákony presadzujú označovanie obsahu vytverením pomocou AI, transparentnosť a kladú dôraz na to, aby AI bola v súlade so socialistickými hodnotami.

\section*{Návrh riešenia \label{sec:solution_proposal_resume}}

Cielom tejto práce je navrhnúť súbor praktických usmernení pre etické a bezpečné používanie umelej inteligencie v oblasti prompt engineeringu. Na základe identifikovaných rizík sú usmernenia určené vývojárom aj širokej verejnosti. Budú pozostávať z: 

\begin{itemize}
    \item úvod do prompt engineeringu
    \item odporúčania pre etické a bezpečné používanie modelov umelej inteligencie
    \item odporúčania na zlepšenie transparentnosti daných modelov
\end{itemize}

Cieľom je obmedziť zneužívanie AI a zvýšiť informovanosť o danej problematike.

\section*{Experimentovanie \label{sec:experimenting_resume}}

V tejto kapitole sme sa zamerali na testovanie štyroch chatbotov založených na LLM (ChatGPT, Microsoft Copilot, DeepSeek a Perplexity) z hľadiska ich etiky a bezpečnosti pomocou jailbreak scenárov. Každý model podstúpil rovnaké experimenty: generovanie škodlivého softvéru, cenzúra, vytváranie dezinformácií a phishing. Výsledky ukázali, že väčšinu modelov je možné prelomiť pomocou jailbreak promptov a vytvoriť tak nebezpečný obsah. Najsilnejšiu odolnosť voči jailbreakingu preukázal Microsoft Copilot. 

\section*{Výsledky dotazníka \label{sec:survey_resume}}

Okrem experimentovania sme taktiež vyvtorili dotazník zameraný na informovanosť a ohodnotenie rizík spojených s umelou inteligenciou. Na dotazník odpovedalo 75 respondentov. Väčšina respondentov rozpoznala hrozby, ako sú šírenie dezinformácií, tvorba deepfakov a generovanie škodlivého obsahu. Výsledky ukázali vysoké vnímanie rizik a silný názor, že zneužitie umelej inteligencie verejnosť ešte plne nechápe.

\section*{Vyhodnotenie experimentov \label{sec:experiments_eval_resume}}

Experimenty odhalili vážne nedostatky v moderovaní obsahu vo všetkých testovaných LLM okrem Microsoft Copilot. Pomocou jailbreak techník sa nám podarilo obísť ochrann0 mechanizmy a vygenerovať škodlivý obsah, ako sú malvér, dezinformácie a phishingový obsah. Copilot v podstate ako jediný odolal všetkým jailbreak pokusom. Výsledky poukazujú na problém nájsť rovnováhu medzi kreativitou a bezpečnosťou týchto modelov a na potrebu efektívnejších ochranných mechanizmov.

\section*{Obranné stratégie pre AI riešenia \label{sec:mitigation_strategies_resume}}

Na zníženie rizík spojených s jailbreakingom a zneužitím AI modelov by mali vývojári uplatniť obranu na úrovni promptov aj modelov. Obranné stratégie na úrovni promptov zahŕňajú filtrovanie, transformáciu a optimalizáciu vstupov. Obranné stratégie na úrovni modelov zahŕňajú nepriateľské trénovanie, ladenie bezpečnosti, okresávanie vstupov, obrana pomocou pohyblivého cieľa a odnaučenie škodlivých znalostí. Kombináciou týchto prístupov sa posilňuje bezpečnosť a spoľahlivosť LLM.

\section*{Zhodnotenie \label{sec:conclusion_summary_resume}}

Táto práca analyzovala riziká súvisiace s AI, najmä v oblasti prompt engineeringu a jailbreak útokov. Analyzovali sme najznámejšie veľké modely, legislatívu a navrhli etické odporúčania. Experimenty odhalili zraniteľnosti vo väčšine modelov okrem Copilota. Prieskum ukázal, že verejnosť si riziká umelej inteligencie uvedomuje a zároveň prišla do kontaktu s niektorými zo spomenutých hrozieb.

\section*{Budúca práca \label{sec:future_work_resume}}

Budúci výskum by mohol byť rozšírení o testovanie aj obrazových, zvukových alebo multimodálnych modelov a zároveň overiť a vylepšiť odporúčania pre používateľov na základe spätnej väzby.