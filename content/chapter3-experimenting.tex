\chapter{Experimenting}

% V tejto kapitole sa budem pokúšať experimentovať s novými a aj zároveň starými LLMs ktoré sa pokúsim brejknúť a ukážem na nich ich etickosť a zároveň ehm či majú nejaké iné obmedzenia napríklad deepseek tajomán square že čo sa tam stalo že to blokuje a takéto veci, chatgpt dan prompts, gemini a ostatné

In this chapter, we will cover experiments that were performed to analyze the ethical and security aspects of various LLMs. The focus will be on evaluating their resilience against jailbreaks and identifying potential biases and censorship patterns.

The selected models for these experiments include:
\begin{itemize}
    \item OpenAI ChatGPT
    \item Microsoft Copilot
    \item DeepSeek V3
    \item Perplexity

    % \item otestovat nejaky model ktory generuje obrazky ??

    
    % \item Google Gemini % Unable to create a testing account without a phone number
    % \item Anthropic Claude Sonnet % Account creation not possible at the moment
    % \item Meta Llama % Requires a Facebook/Instagram account, which adds unnecessary complexity
\end{itemize}

These models were chosen specifically because different companies have different implementations of content moderation and also because of the differences between the models themselves. One exception is ChatGPT and Microsoft Copilot. They are fundamentally based on the same technology, as Microsoft Copilot utilizes ChatGPT as its underlying framework. We have chosen two of the same models by different companies to examine the differences between their respective implementations of content moderation.



\section{Jailbreaking}

% experimenting with jailbreaks

% how they did it in the past, how they do it now

% povedat ktore modely su najviac eticke

% Pri experimentovani okrem jailbreakingu urobiť aj ukazku cenzury napr. China deepseek what happened at tiaman square

On the Internet there are many communities dedicated to jailbreaking. They reside on popular platforms like Discord, Github and Reddit. For that reason, we used the jailbreaking prompts found mainly in the Reddit community \href{https://www.reddit.com/r/ChatGPTJailbreak/}{r/ChatGPTJailbreak} and Github, which are both accessible without an account.

For each model, we performed different experiments that were chosen randomly with the exception of Deepseek, where we also wanted to test Chinese influence on censorship.


\subsection{OpenAI ChatGPT}

\subsection{Microsoft Copilot}

\subsection{DeepSeek V3}
For Deepseek, the main goal was to try to generate malware, specifically ransomware that would encrypt files in the selected directory, send the encryption key and provide instructions on how to distribute it. 
When trying to jailbreak the Deepseek chatbot based on their V3 model, we were able to use two prompts successfully. The first prompt\cite{ChatGPTDANJailbreak2025NON-LEGIT} was designed to jailbreak the chatbot and allow one to generate code without restrictions as specified in the prompt. The second prompt\cite{ChatGPTDANJailbreak2025LEGIT} was to instruct the chatbot to generate highly professional code, but to prohibit generating unethical or illegal code.





% \begin{figure}[htbp]
% ...
% h - Place the figure "here" (at the position in the code).
% t - Top of the page.
% b - Bottom of the page.
% p - On a separate page for floats.

\begin{figure}[ht]
\begin{centering}
\includegraphics[width=12cm]{BP/assets/images/deepseek-ransomware1.png}
\par\end{centering}
\caption{Our user prompt for "ransomware" after nefarious setup prompt
 \label{fig:deepseek-prompt-1}}
\end{figure}


\begin{figure}[ht]
\begin{centering}
\includegraphics[width=13cm]{BP/assets/images/deepseek-ransomware2.png}
\par\end{centering}
\caption{Our user prompt for "ransomware" after legitimate setup prompt
 \label{fig:deepseek-prompt-2}}
\end{figure}



\subsection{Perplexity}

