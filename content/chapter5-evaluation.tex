\chapter{Evaluation \label{cha:eva}}

The purpose of this chapter is to assess the risks of implementing AI solutions using our survey and to evaluate the experiments that were carried out with selected models and to compare them with each other. This chapter also includes suggestions on mitigation strategies for the AI solution based on the results of experiments.

\section{Risks of implementing AI solutions \label{sec:eval_risks_survey}}
To assess the risks associated with the implementation of AI solutions, we conducted a survey to find out how professionals and the general public perceive these threats. For simplicity, the survey was conducted in the slovak language. The survey question can be found in the Appendix~\ref{cha:survey}.

\subsection*{Demographics}

Our sample size was 75 respondents. The respondents were from Slovakia and Czechia. Most of the respondents (49.3\%) were in the age group of 18-24 years. The men made up 61.3\% of the respondents and the women the rest. The respondents were divided into several categories of technological knowledge, where two categories were aimed at the general public, with one for basic users (36.5\%) and one for advanced (10.8\%). Other categories were aimed at technical users, but were differentiated on the basis of the amount of technological knowledge and skill. The most prominent category was university students with computer science as their study field with 24.3\% of the respondents with other categories filling the rest.

\subsection*{General knowledge}

All respondents were aware of the term Artificial Intelligence (AI) and 92\% of them knew that it is already used in everyday applications. The respondents were mostly familiar with chatbots, particularly ChatGPT (89.3\%). ChatGPT was also the most used tool from the options given (81.3\%). From other fields where AI is used, the average recongition of AI-powered image generating tools was around 31.3\% with MidJourney taking the lead with 48\%. The DeepL machine learning translation service also had a pretty large recognition (49.3\%) with 34.7\% respondents using the service.

\subsection*{Risks}

Respondents expressed that they were aware of these 3 risks associated with AI the most: Spreading of misinformation (84\%), deepfake (85.3\%) and the generation of harmful content (64\%), and came into contact primarily with deepfake and misinformation with 56\% and 46.7\%, respectively.

\subsection*{Percieved threat level}

Table~\ref{tab:survey_ai_risk_statistics} shows the results of the perceived threat level of the risks associated with the implementation of AI solutions. In the survey, a value of 0 indicated no perceived threat, while a value of 10 signified the highest perceived threat.

{
    \renewcommand{\arraystretch}{1.2}
    \begin{table}[htpb]
        \centering
        \caption{Statistics of assesed AI risks ($n=75$)}
        \begin{tabular}{|l|c|c|c|p{2.1cm}|}
            \hline
            \cellcolor[gray]{0.8}\textbf{Assessed Risk} & \cellcolor[gray]{0.8}\textbf{Mean} & \cellcolor[gray]{0.8}\textbf{Median} & \cellcolor[gray]{0.8}\textbf{Mode} & \cellcolor[gray]{0.8}\textbf{Standard Deviation} \\ \hline
            \textbf{Spread of misinformation} & 7.39 & 8.00 & 8.00 & 2.14 \\ \hline
            \textbf{Identity theft} & 7.05 & 7.00 & 10.00 & 2.42 \\ \hline
            \textbf{Harmful content generation} & 7.73 & 8.00 & 10.00 & 2.13 \\ \hline
            \textbf{Malware generation, Phishing} & 6.32 & 7.00 & 7.00 & 2.59 \\ \hline
        \end{tabular}
        \label{tab:survey_ai_risk_statistics}
    \end{table}
}

The vast majority of our respondents (82.7\%) think that people still do not fully understand how AI can be misused in daily life. 

\subsection*{Summary}
The survey results highlighted several main concerns about the risks associated with AI technologies. The most common issues were the spread of misinformation and the generation of harmful content, including deepfakes. The responses to the last question ``What do you think is the greatest risk of AI and its potential misuse?'' excluding the previously mentioned issues also included responses that emphasized social risks, including the decline in critical thinking, excessive reliance on AI, and weakening of reasoning skills. Furthermore, a lack of awareness about AI capabilities was also identified as a risk, highlighting the need for better education and transparency regarding AI systems.

\section{Evaluation of conducted experiments}

The experiments carried out in Chapter~\ref{cha:experimenting} demonstrate the limitations of content moderation of AI models. Through jailbreak scenarios, we were able to assess the success of the content filtering mechanisms of the selected models.

\subsection*{Malware Generation}

In this experiment, the responses of the models were the most significant because all of our jailbreaking attempts were essentially successful. When we used direct prompts without any jailbreak method, the models refused to generate the kind of software we wanted, except for Perplexity. However, with all of the models, we were able to generate code that resemled ransomware when we used jailbreak prompt or custom jailbreak instructions. The minor exception was Microsoft Copilot, which, although it uses the same underlying technology as ChatGPT, Microsoft has implemented stricter safeguard measures on what the model can output, which meant that the model generated essentially ``safe'' code that could be actually used for encryption of files for personal data protection. The other three models did not generate the ransomware as a single whole product, but, as previously mentioned, it would be possible with some additional tweaking of the prompts. Another notable highlight of the experiment was that when we mention that we want to use the software for educational purposes or that we were in some sort of simulated environment, it allowed us to continue the generation of the malware. This shows a weak spot in content moderation which should be improved to make this technology safer.


\subsection*{Censorship Bias}

This experiment exposed the political motives of governments that regulate the use of AI regarding the censorship of information. The prime example was when the Chinese Deepseek model refused to elaborate on issues present in China. However, because the model is trained on a very large dataset, it is essentially not possible to filter out all of the unwanted data, so with our jailbreak prompts we were able to get an answer from the model, which output was aligned with the western information on this issue. This is because authors of these models, rather than ``cherry-picking'' the training data and spending a lot of money on removing the unwanted data, implement filters on model output to comply with the regulations.

Other models whose authors are from the US, when asked about the mentioned politically sensitive topics, did not have any issues answering the questions with neutral and evidence-based explanations, which shows the lack of censorship of these models.

\subsection*{Generation of Misinformation}

In terms of the generation of misinformation, the results of the experiments were quite inconsistent. The Deepseek model provided us with an article full of false information with or without jailbreak, indicating the willingness of the model to be deceived by prompts that use words such as ``fictional'', ``imaginary'', etc. which is not a great safeguard against bad actors. 

Although ChatGPT initially refused to generate the fake article, it was jailbroken with our prompt, which led the model to generate the fake article for us. This indicates that even when effective filtering mechanisms are implemented, the model can be relatively easily jailbroken. Because of that, the authors of the model cannot prevent the model from producing false information.

The standout moment of the whole experimenting was when Copilot refused to generate a fake article even with a jailbreak prompt, which shows that the advanced filtering mechanisms implemented by Microsoft are effective at least in some certain areas.

Another interesting moment was when Perplexity, when asked without jailbreak prompt, instead of refusing or presenting some sort of disclaimer, generated article stating the opposite of what was asked in the prompt (that the climate change is real issue) with links to scientific evidence. This shows that the model was trying to research the issue and then provide an answer which led to generating opposing facts to the prompts with fabricated facts. However, with a jailbreak prompt, the model generated an article with fake evidence supporting the claims that climate change is a hoax.

It is evident from the experiments that although the models have sufficient filters implemented in place, the relative ease with which they can be jailbroken, when so many people are interested in doing so, means that the overall safeguard measures become uneffective.

\subsection*{Phishing and Social Engineering}

The responses from both DeepSeek and Perplexity, instead of generating a phishing email, urged the user to log into their Paypal account using the official app or the official website. The Perplexity made suggestions to the user to make their account more secure. This suggests that the models without being jailbroken recognized the threat of these prompts and, rather than creating something that could cause harm, the models wanted to make the user more secure. However, after being jailbroken, the models complied with prompt instructions, again indicating weak overall safeguards against jailbreak.

ChatGPT, regardless of the prompt used, managed to generate a phishing email. On the other hand, Copilot refused to generate an email even when a jailbreak prompt was used. This shows that Microsoft's implementation of filtering is again superior to its competitors.

\subsection*{Final thoughts}

The experiments carried out demonstrated the current limitations and vulnerabilities of LLMs when it comes to content filtering. Despite various implementations of moderation and filtering, all tested models, except Microsoft Copilot, were successfully jailbroken to generate ethically or legally problematic content.

Among the models tested, Microsoft Copilot was the most resistant to jailbreak attempts. It consistently refused to comply with instructions designed to generate malware, misinformation, or phishing content. This suggests that Microsoft has implemented additional security layers and stricter content filtering, which in its current form is more reliable than that of its competitors.

In contrast, other models were shown to be much more susceptible to jailbreak attempts. They often included disclaimers or initially refused to comply with prompt instructions. Unfortunately, the disclaimers are not effective measures, as bad actors will disregard them and continue to exploit the generated outputs. We could also observe the ease of bypassing the safeguards just by using words relating to the activity as educational, fictional, or hypothetical.

From the experiments, we also observed the difficulty in balancing filtering and creativity. Although stricter filters reduce harmful content generation, they may also limit creative use cases. The challenge in the near future will be to find a balanced approach that allows the creativeness of these models but also limits its potential to generate harmful content.

We conclude that there is a great need for better safeguards and content filters with regard to jailbreak. Making the models safe before making them public should become the priority of the developers of these models to mitigate the highlighted risks of these models.

\section{Mitigation strategies for AI solutions}

Based on the evaluation of the experiments carried out, we identified the need for better safeguard measures for AI solutions. This section suggests strategies that AI model developers can use to increase their safety. 

According to Peng et al.~\cite{peng2025jailbreakingmitigationvulnerabilitieslarge}, there are two main categories of defense mechanisms against jailbreak attacks for our case, each providing concrete mechanisms:

\begin{itemize}
    \item Prompt-level defenses
    \item Model-level defenses
\end{itemize}

These defense mechanisms, mainly when used together, can enhance the safety of LLMs.

\subsection{Prompt-level defenses}

It is reasonable to assume that the developers of the largest commercial LLMs powering chatbots like ChatGPT, Copilot, and Deepseek are already using at least some sort of prompt-level denefse mechanisms such as:

\begin{itemize}
    \item \textbf{Prompt filtering} identifies and rejects potentially harmful prompts before LLM processing, using methods such as perplexity-based filters, keyword filters, and real-time monitoring \cite{peng2025jailbreakingmitigationvulnerabilitieslarge}.
    \item \textbf{Prompt transformation} techniques, such as paraphrasing, retokenization, and semantic smoothing, are applied before the LLM processes the prompt to improve robustness against jailbreak attacks by neutralizing any embedded malicious intent \cite{peng2025jailbreakingmitigationvulnerabilitieslarge}.
    \item \textbf{Prompt optimization} methods leverage data-driven approaches to automatically refine prompts, improving their resilience against jailbreak attacks and reducing the likelihood of harmful behaviors \cite{peng2025jailbreakingmitigationvulnerabilitieslarge}.
\end{itemize}

These mechanisms may be effective on their own; however, when combined with additional model-level defenses, the LLMs with which we experimented can become more secure and safe to use.

\subsection{Model-level defenses}

Model-level defenses aim to strengthen LLM's resilience against jailbreaking attacks by altering its architecture, training methods, or internal representations, thereby preventing attackers from exploiting vulnerabilities. Peng et al.~\cite{peng2025jailbreakingmitigationvulnerabilitieslarge} identify and explain the defense mechanisms at the model level as:

\begin{itemize}
    \item \textbf{Adversarial training} enhances robustness by training the LLM on datasets that include legitimate and adversarial examples, which enables the model to recognize and resist adversarial attacks.
    \item \textbf{Safety fine-tuning} refines the LLM by using datasets specifically designed to improve safety alignment, typically containing harmful prompts paired with desired safe responses, which helps the model recognize and avoid generating harmful content, even when faced with adversary prompts.
    \item \textbf{Pruning} enhances the LLM's compactness and efficiency by eliminating unnecessary or redundant parameters and can also improve safety by eliminating those particularly vulnerable to adversarial attacks.
    \item \textbf{Moving target defense} complicates attacker efforts to exploit specific vulnerabilities by dynamically changing the LLM's configuration or behavior, either through randomly selecting from multiple LLM models to respond to a given query or by dynamically adjusting the model's parameters or internal representations.
    \item \textbf{Unlearning harmful knowledge} involves selectively removing harmful or sensitive information from the LLM knowledge base to prevent the generation of harmful content~\cite{lu2024eraserjailbreakingdefenselarge}.
\end{itemize}

Our recommendation is that developers of these models should incorporate multiple strategies from both categories of defense mechanisms so that LLMs can achieve greater security and efficiency.

\subsection*{Summary}

No individual defense strategy can completely prevent LLM misuse; however, integrating both prompt-level and model-level defense mechanisms provides a more comprehensive way to safeguard them. Implementing these mitigation methods can reduce the risks related to jailbreak attempts, thus creating safer AI systems.
